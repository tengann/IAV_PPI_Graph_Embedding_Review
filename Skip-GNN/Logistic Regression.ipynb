{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Logistic Regression.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPgR3Lpc6QjnMtnwztIKgvL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"AEBqB7p3NvZz"},"outputs":[],"source":["## NN classifier (keras.models.Sequential)\n","## Recurrent neural network (RNN) - e.g. LSTM keras.layers"]},{"cell_type":"code","source":["## Graph embedding, Protein seq embedding, Structure embedding, RDKit (??)"],"metadata":{"id":"tP7n-idxPP_k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B-0Lh4rgE2N8","executionInfo":{"status":"ok","timestamp":1660293655582,"user_tz":-480,"elapsed":17037,"user":{"displayName":"ann ng","userId":"16723765352528695909"}},"outputId":"b477bc4f-a118-42c3-de5d-314c9037a4c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Load Path"],"metadata":{"id":"552m8MCUE3_N"}},{"cell_type":"code","source":["## Load Path\n","get_path = 'drive/MyDrive/Skip-GNN/'\n","data_path = get_path + 'data/IAV/'"],"metadata":{"id":"8ygDwFuoE3kU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Import Libaries"],"metadata":{"id":"NH7_n3GKE508"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","## Pre-processing\n","import scipy.sparse as sp\n","from scipy import sparse\n","\n","## Logistic Regression\n","from sklearn.linear_model import LogisticRegression\n","\n","## NN Classifier\n","## Sequential model: Plain stack of layers where each layer has exactly one input tensor and one output tensor\n","from keras.models import Sequential\n","from keras.layers import Dense,Activation,Dropout\n","from keras import callbacks ## Early Stopping\n","\n","from sklearn.utils import shuffle\n","\n","from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, f1_score"],"metadata":{"id":"6DOSknNERAL0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["protein_list = pd.read_csv(data_path + 'protein_list.csv')\n","# protein_list.columns = ['idx', 'Protein1_ID']\n","## protein_list"],"metadata":{"id":"2KU1Aif6MFez"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Utils"],"metadata":{"id":"v18Zes8DW3z_"}},{"cell_type":"markdown","source":["### Metrics"],"metadata":{"id":"QgKoqDq3QrGU"}},{"cell_type":"code","source":["## Add F1, ROC-AUC and PR-AUC\n","\n","def calculate_metrics(y_label, y_pred, y_pred_f):\n","\n","    # For binary classification\n","    TP,FP,TN,FN = 0,0,0,0\n","    for i in range(len(y_label)):\n","        if y_label[i] == y_pred[i]:\n","            if y_label[i] == 1:\n","                TP = TP + 1\n","            else:\n","                TN = TN + 1\n","        else:\n","            if y_pred[i] == 1:\n","                FP = FP + 1\n","            else:\n","                FN = FN + 1\n","    \n","    accuracy = (TP + TN) / float(TP + TN + FP + FN)\n","    sensitivity = TP / float(TP + FN)\n","    specificity = TN / float(TN + FP)\n","    precision = TP / float(TP + FP)\n","    F1 = (2 * precision * sensitivity) / (precision + sensitivity)\n","\n","    ROC_AUC = roc_auc_score(y_label, y_pred_f)\n","    PR_AUC = average_precision_score(y_label, y_pred_f)\n","\n","    return [accuracy, sensitivity, specificity, precision, F1, ROC_AUC, PR_AUC]"],"metadata":{"id":"CTofFEKxQp8z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Input data"],"metadata":{"id":"U3WWGUnlW64e"}},{"cell_type":"code","source":["def create_fold():\n","\n","  ## hold-out test set\n","  test_df = pd.read_csv(data_path + 'edges/nn_test_idx.csv')\n","  pos_test = test_df[test_df['label'] == 1]\n","  neg_test = test_df[test_df['label'] == 0]\n","\n","  ## Training and Validation Set\n","  pos_org = pd.read_csv(data_path + 'edges/nn_pos_idx.csv')\n","  neg_org = pd.read_csv(data_path + 'edges/nn_neg_idx.csv')\n","\n","  '''\n","    Shuffle Data\n","  '''\n","  pos = shuffle(pos_org)\n","  neg = shuffle(neg_org)\n","\n","  ## Train : Val = 9 : 1\n","  pos_val = pos.sample(frac = 0.1, replace = False) ## 10% of positive dataset\n","  pos_train = pos[~pos.index.isin(pos_val.index)]\n","\n","  neg_val = neg.sample(frac = 0.1, replace = False) ## 10% of negative dataset\n","  neg_train = neg[~neg.index.isin(neg_val.index)]\n","\n","  # train_df = pd.concat([pos_train, neg_train], ignore_index=True)\n","  # val_df = pd.concat([pos_val, neg_val], ignore_index=True)\n","\n","  print('--Sampled new data--')\n","\n","  # return train_df, val_df, test_df\n","  return pos_train, neg_train, pos_val, neg_val, pos_test, neg_test ## train_edges, train_edges_false, val_edges, val_edges_false, test_edges, test_edges_false"],"metadata":{"id":"mTpF8ijxEy1L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Read embeddings of all proteins\n","\n","def read_embedding_matrix(emb_mtd):\n","  \n","  protein_list_len = len(protein_list)\n","\n","  ## Read embedding\n","  edit_data_path = data_path + 'graph_embeddings/SDNE/'\n","  # emb = pd.read_csv(edit_data_path + emb_mtd + '.csv', skiprows=1, header = None).sort_values(by = [0]).set_index([0]) \n","  emb = pd.read_csv(edit_data_path + emb_mtd + '.txt', sep=' ', skiprows=1, header = None).sort_values(by = [0]).set_index([0]) ## SDNE, GraRep\n","\n","  ## Convert csv to array\n","  for i in np.setdiff1d(np.arange(protein_list_len), emb.index.values): ## setdiff1d: 1D array of values in ar1 that are not in ar2\n","      emb.loc[i] = (np.sum(emb.values, axis = 0)/emb.values.shape[0]) ## manually insert emb for protein indexes with no node2vec embedding\n","\n","  features = emb.sort_index().values\n","\n","  print(\"---Read Embeddings---\")\n","  print(features.shape)\n","  # print(features)\n","\n","  return features"],"metadata":{"id":"TgiMT9kxL3n0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Retrieve embeddings"],"metadata":{"id":"sN7REN3__vHv"}},{"cell_type":"code","source":["## Retrieve embeddings w.r.t X_train, y_train, X_test, y_test\n","def retrieve_edge_embeddings(emb, posEdges, negEdges):\n","  X = []\n","  Y = []\n","\n","  for i in posEdges.index:\n","    node_u_emb = emb[posEdges['Protein1_ID'][i]]\n","    node_v_emb = emb[posEdges['Protein2_ID'][i]]\n","    feature_vector = np.append(node_u_emb, node_v_emb)\n","    X.append(feature_vector)\n","    Y.append(1)\n","  \n","  for i in negEdges.index:\n","    node_u_emb = emb[negEdges['Protein1_ID'][i]]\n","    node_v_emb = emb[negEdges['Protein2_ID'][i]]\n","    feature_vector = np.append(node_u_emb, node_v_emb)\n","    X.append(feature_vector)\n","    Y.append(0)\n","\n","  # print(\"---Generate data---\")\n","  # # print(X) ## list\n","  # print(len(X)) \n","  # # print(Y)\n","  # print(len(Y))\n","\n","  return X, Y"],"metadata":{"id":"SRVVkVsd9zfx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Logistic Regression"],"metadata":{"id":"gIU4IJMf5aGA"}},{"cell_type":"code","source":["def trainLG(X_train, y_train, X_val, y_val, X_test, y_test):\n","  \n","  clf1 = LogisticRegression(max_iter=3000)\n","  clf1.fit(X_train, y_train)\n","\n","  y_pred_proba = clf1.predict_proba(X_test)[:, 1]\n","  y_pred = clf1.predict(X_test)\n","\n","  auc_roc = roc_auc_score(y_test, y_pred_proba)\n","  auc_pr = average_precision_score(y_test, y_pred_proba)\n","  accuracy = accuracy_score(y_test, y_pred)\n","  f1 = f1_score(y_test, y_pred)\n","\n","  acc = calculate_metrics(y_test, y_pred, y_pred_proba)\n","\n","  return acc"],"metadata":{"id":"IneI2xUh5ZuA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## main"],"metadata":{"id":"DOsvOYfZQyi9"}},{"cell_type":"code","source":["def train(input_type):\n","\n","    ## Call create_fold()\n","    train_edges, train_edges_false, val_edges, val_edges_false, test_edges, test_edges_false = create_fold()\n","\n","    print(train_edges.shape,train_edges_false.shape)\n","    print(val_edges.shape,val_edges_false.shape)\n","    print(test_edges.shape,test_edges_false.shape)\n","\n","    ## Retrieve embeddings of all proteins \n","    emb = read_embedding_matrix(input_type) ## embedding method\n","\n","    ## Retrieve embeddings of respective nodes \n","    X_train,Y_train = retrieve_edge_embeddings(emb, train_edges, train_edges_false)\n","    X_val, Y_val = retrieve_edge_embeddings(emb, val_edges, val_edges_false)\n","    X_test,Y_test = retrieve_edge_embeddings(emb, test_edges, test_edges_false)\n","  \n","    ## Final softmax classifier\n","    # acc = train_nn(X_train,Y_train,X_val,Y_val,X_test,Y_test)\n","    acc = trainLG(X_train, Y_train, X_val, Y_val, X_test, Y_test)\n","\n","    return acc"],"metadata":{"id":"yl6EMf9DQEXO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## TEST RUN\n","# acc = train(input_type='node2vec')\n","# print(acc)"],"metadata":{"id":"L2he5SfMhA4n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## 5-fold Cross Validation\n","eval_metrics = []\n","\n","for i in range(0, 5):\n","\n","    print('Iteration(train): ', (i+1))\n","    acc = train(input_type='a0_b0') ## Change input type\n","\n","    print('---Link Prediction Performance---')\n","    print(acc)\n","   \n","    eval_metrics.append(acc)\n","    # print(eval_metrics)"],"metadata":{"id":"UOa130vYXyuW","executionInfo":{"status":"ok","timestamp":1660293732856,"user_tz":-480,"elapsed":68334,"user":{"displayName":"ann ng","userId":"16723765352528695909"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a94ada2e-c6f3-4bac-9ceb-c648518d9390"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration(train):  1\n","--Sampled new data--\n","(3422, 3) (3422, 3)\n","(380, 3) (380, 3)\n","(422, 3) (422, 3)\n","---Read Embeddings---\n","(15685, 128)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"]},{"output_type":"stream","name":"stdout","text":["---Link Prediction Performance---\n","[0.9786729857819905, 0.9834123222748815, 0.9739336492890995, 0.9741784037558685, 0.9787735849056605, 0.997748253633117, 0.997697972768367]\n","Iteration(train):  2\n","--Sampled new data--\n","(3422, 3) (3422, 3)\n","(380, 3) (380, 3)\n","(422, 3) (422, 3)\n","---Read Embeddings---\n","(15685, 128)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"]},{"output_type":"stream","name":"stdout","text":["---Link Prediction Performance---\n","[0.9774881516587678, 0.981042654028436, 0.9739336492890995, 0.9741176470588235, 0.9775678866587957, 0.9975236405291885, 0.9974352657727892]\n","Iteration(train):  3\n","--Sampled new data--\n","(3422, 3) (3422, 3)\n","(380, 3) (380, 3)\n","(422, 3) (422, 3)\n","---Read Embeddings---\n","(15685, 128)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"]},{"output_type":"stream","name":"stdout","text":["---Link Prediction Performance---\n","[0.9786729857819905, 0.9834123222748815, 0.9739336492890995, 0.9741784037558685, 0.9787735849056605, 0.9975573324947777, 0.9974794944396794]\n","Iteration(train):  4\n","--Sampled new data--\n","(3422, 3) (3422, 3)\n","(380, 3) (380, 3)\n","(422, 3) (422, 3)\n","---Read Embeddings---\n","(15685, 128)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"]},{"output_type":"stream","name":"stdout","text":["---Link Prediction Performance---\n","[0.9786729857819905, 0.9834123222748815, 0.9739336492890995, 0.9741784037558685, 0.9787735849056605, 0.9978437142022866, 0.99779684106588]\n","Iteration(train):  5\n","--Sampled new data--\n","(3422, 3) (3422, 3)\n","(380, 3) (380, 3)\n","(422, 3) (422, 3)\n","---Read Embeddings---\n","(15685, 128)\n","---Link Prediction Performance---\n","[0.9774881516587678, 0.981042654028436, 0.9739336492890995, 0.9741176470588235, 0.9775678866587957, 0.9976752543743401, 0.9976074775543173]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"]}]},{"cell_type":"markdown","source":["## Evaluation"],"metadata":{"id":"AsuIy6imZHRJ"}},{"cell_type":"code","source":["## Append results here\n","eval_arr = eval_metrics"],"metadata":{"id":"TosCbYZ_ZNN3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mean = np.array(eval_arr).mean(axis=0) # Take the mean of each column\n","mean = np.round(mean, 4)\n","print('Mean: ' + str(mean)[1:-1])\n","\n","max = np.array(eval_arr).max(axis=0)\n","max = np.round(max, 4)\n","print('Max: ' + str(max)[1:-1])\n","\n","min = np.array(eval_arr).min(axis=0)\n","min = np.round(min, 4)\n","print('Min: ' + str(min)[1:-1])"],"metadata":{"id":"6CKSmq2wZNwn","executionInfo":{"status":"ok","timestamp":1660293733410,"user_tz":-480,"elapsed":8,"user":{"displayName":"ann ng","userId":"16723765352528695909"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2c73782b-1d3f-4eb8-8250-754a6dc74574"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean: 0.9782 0.9825 0.9739 0.9742 0.9783 0.9977 0.9976\n","Max: 0.9787 0.9834 0.9739 0.9742 0.9788 0.9978 0.9978\n","Min: 0.9775 0.981  0.9739 0.9741 0.9776 0.9975 0.9974\n"]}]},{"cell_type":"code","source":["## CHANGE METHOD HERE\n","mtd_name = 'SDNE-a0_b0'"],"metadata":{"id":"ZabcB1fIZTq8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(get_path + 'NN_results.txt', \"a\") as f:\n","    f.write(mtd_name + ': ' + str(mean) + '\\n')"],"metadata":{"id":"JHTGkazoZeb9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Additional codes"],"metadata":{"id":"EXrNQqzBAfiX"}},{"cell_type":"code","source":["### Map edges to index in protein list\n","\n","# pos_org = pd.read_csv(data_path + 'edges/train_val_pos_tune_hyperparams.csv')\n","# pos_org_m1 = pd.merge(pos_org, protein_list, on=['Protein1_ID'])\n","# pos_org_m1 = pd.merge(pos_org_m1, protein_list, left_on=['Protein2_ID'], right_on=['Protein1_ID'])\n","# # pos_org_m1\n","\n","# pos_org_f = pos_org_m1[['idx_x', 'idx_y', 'label']]\n","# pos_org_f\n","# # pos_org_f.to_csv(data_path + 'edges/nn_pos_idx.csv', index=False, header=['Protein1_ID', 'Protein2_ID', 'label'])\n","\n","# neg_org = pd.read_csv(data_path + 'edges/train_val_neg_tune_hyperparams.csv')\n","# neg_org_m1 = pd.merge(neg_org, protein_list, on=['Protein1_ID'])\n","# neg_org_m1 = pd.merge(neg_org_m1, protein_list, left_on=['Protein2_ID'], right_on=['Protein1_ID'])\n","\n","# neg_org_f = neg_org_m1[['idx_x', 'idx_y', 'label']]\n","# neg_org_f\n","# # neg_org_f.to_csv(data_path + 'edges/nn_neg_idx.csv', index=False, header=['Protein1_ID', 'Protein2_ID', 'label'])\n","\n","## hold-out test set\n","# test_df = pd.read_csv(data_path + 'edges/test_holdout_tune_hyperparams.csv')"],"metadata":{"id":"zhRKLv6EAi9Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## trainNN\n","# def generate_data(emb, posEdges, negEdges):\n","    \n","#     ## Stack embeddings of two proteins together\n","#     posNum = posEdges.shape[0]\n","#     negNum = negEdges.shape[0]\n","\n","#     X = np.empty((posNum+negNum,2*emb.shape[1]))\n","#     k = 0\n","\n","#     # for x in posEdges.index:\n","#     #     X[k] = np.hstack((emb[x[0]],emb[x[1]]))\n","#     #     k = k + 1\n","#     # for x in negEdges.index:\n","#     #     X[k] = np.hstack((emb[x[0]],emb[x[1]]))\n","#     #     k = k + 1 \n","\n","#     for i in posEdges.index:\n","#        X[k] = np.hstack((emb[posEdges['Protein1_ID'][i]], emb[posEdges['Protein2_ID'][i]]))\n","#        k = k + 1\n","    \n","#     for i in negEdges.index:\n","#       X[k] = np.hstack((emb[negEdges['Protein1_ID'][i]], emb[negEdges['Protein2_ID'][i]]))\n","#       k = k + 1\n","\n","#     Y_pos = np.full((posNum,2),[0,1])\n","#     Y_neg = np.full((negNum,2),[1,0])\n","#     Y = np.vstack((Y_pos,Y_neg))\n","\n","#     print(\"---Generate data---\")\n","#     print(X)\n","#     print(X.shape) ## (2D array)\n","#     print(Y) ## Class label (2D array)\n","#     print(Y.shape)\n","\n","#     return X,Y"],"metadata":{"id":"zS9u5qwWAnc9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### trainNN Model"],"metadata":{"id":"oR6I9TYdQ5f9"}},{"cell_type":"code","source":["# def train_nn(X_train, Y_train, X_val, Y_val, X_test, Y_test): \n","\n","#     model = Sequential()\n","#     model.add(Dense(128, activation='relu', input_dim=X_train.shape[1]))\n","#     model.add(Dropout(0.5))\n","#     model.add(Dense(64, activation='relu'))\n","#     model.add(Dropout(0.5))\n","#     model.add(Dense(32, activation='relu'))\n","#     model.add(Dropout(0.5))\n","#     model.add(Dense(2,activation='softmax'))\n","\n","#     model.compile(loss='categorical_crossentropy',\n","#                     optimizer='adam',\n","#                     metrics=['accuracy'])\n","\n","#     ## Early stopping on validation dataset (10% of overall dataset)\n","#     earlystopping = callbacks.EarlyStopping(monitor='val_loss', mode=\"min\", patience=5, restore_best_weights=True)\n","#     model.fit(X_train, Y_train, epochs=200, batch_size=128, validation_data=(X_val, Y_val), callbacks=[earlystopping])\n","\n","#     ## Save model to make future predictions\n","#     model.save(get_path + 'NN_model/model.h5')\n","#     print(\"---Saved model to disk---\")\n","\n","#     ### Using Hold-out Test Set (X_test and Y_test)\n","#     # y_prob = model.predict(X_test) \n","#     print(y_prob)\n","#     y_classes = y_prob.argmax(axis=-1) ## Binary prediction (y_pred)\n","\n","#     y_pred_float = model.predict_proba(X_test) ## Float prediction\n","#     y_pred_f = y_pred_float[:,1] ## Probability of belonging to class label '1' \n","#     print(y_pred_float) \n","#     print(y_pred_f)\n","\n","#     y_true = Y_test[:,1]\n","\n","#     acc = calculate_metrics(y_true, y_classes, y_pred_f) ## calculate_metrics(y_label, y_pred, y_pred_f)\n","\n","#     return acc"],"metadata":{"id":"4j4hwrsXBVee"},"execution_count":null,"outputs":[]}]}